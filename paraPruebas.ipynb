{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esta notebook es creada para hacer pruebas con el procesamiento de la informacion de los documentos csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import json\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _para limpiar los documentos extraidos del pdf_\n",
    "    ->> obtener una lista de todas las materias con sus nrc y la carrera en la que se ofertan\n",
    "    --- limpiar la lista de materias (toLower)\n",
    "    --- sustituir los nombres de materias por los correctos\n",
    "    ---reemplazar los NAN donde se genera esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Materia</th>\n",
       "      <th>Clave</th>\n",
       "      <th>Secc</th>\n",
       "      <th>NRC</th>\n",
       "      <th>Dias</th>\n",
       "      <th>Hora</th>\n",
       "      <th>Salon</th>\n",
       "      <th>Profesor</th>\n",
       "      <th>Doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Inteligencia Artificial</td>\n",
       "      <td>CCOM 262</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29089</td>\n",
       "      <td>AJ</td>\n",
       "      <td>0900-1059</td>\n",
       "      <td>1EMA4/409</td>\n",
       "      <td>ZACARIAS - FLORES FERNANDO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Inteligencia Artificial</td>\n",
       "      <td>CCOM 262</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29089</td>\n",
       "      <td>L</td>\n",
       "      <td>1000-1059</td>\n",
       "      <td>1EMA4/409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Arqu. Funcional de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZACARIAS - FLORES FERNANDO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Computadora</td>\n",
       "      <td>IDCO 200</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29093</td>\n",
       "      <td>L</td>\n",
       "      <td>1300-1359</td>\n",
       "      <td>1CCO5/201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Arqu. Funcional de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SANCHEZ - GALVEZ MARIA EUGENIA N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Computadora</td>\n",
       "      <td>IDCO 200</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29093</td>\n",
       "      <td>M</td>\n",
       "      <td>1300-1459</td>\n",
       "      <td>1CCO3/309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Materia     Clave Secc    NRC Dias       Hora      Salon  \\\n",
       "16  Inteligencia Artificial  CCOM 262  OO1  29089   AJ  0900-1059  1EMA4/409   \n",
       "17  Inteligencia Artificial  CCOM 262  OO1  29089    L  1000-1059  1EMA4/409   \n",
       "18       Arqu. Funcional de       NaN  NaN     -1  NaN        NaN        NaN   \n",
       "19              Computadora  IDCO 200  OO1  29093    L  1300-1359  1CCO5/201   \n",
       "20       Arqu. Funcional de       NaN  NaN     -1  NaN        NaN        NaN   \n",
       "21              Computadora  IDCO 200  OO1  29093    M  1300-1459  1CCO3/309   \n",
       "\n",
       "                            Profesor  Doc  \n",
       "16        ZACARIAS - FLORES FERNANDO    0  \n",
       "17                               NaN    0  \n",
       "18        ZACARIAS - FLORES FERNANDO    0  \n",
       "19                               NaN    0  \n",
       "20  SANCHEZ - GALVEZ MARIA EUGENIA N    0  \n",
       "21                               NaN    0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###celda de erorres TODO\n",
    "data[0][16:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recibe un directorio\n",
    "#retorna todos los documentos de ese directorio\n",
    "def getDocsinDir(directory):\n",
    "    return list(map(lambda x: directory+x,os.listdir(directory)))\n",
    "#recibe una lista de nombres de documentos\n",
    "#retorna un lista de dataFrames\n",
    "def getCSVs(docs):\n",
    "    data=[]\n",
    "    for x in docs:\n",
    "        data.append(pd.read_csv(x))\n",
    "    return data\n",
    "#recibe un pandas.Dataframes\n",
    "    #elimina todos los elementos nulos y ajusta los nombres de materias y profesores\n",
    "#retorna el mismo Dataframe  \n",
    "def cleanData(data):    \n",
    "        data = data[data['Materia'].notna()]\n",
    "        data['NRC']=data['NRC'].replace(to_replace=np.nan,value=-1)\n",
    "        #data[2].dropna(inplace=True)\n",
    "        data.reset_index(drop=True,inplace=True)\n",
    "        bad=data.loc[lambda y: y.Materia=='Materia']\n",
    "        #print(bad)\n",
    "        badIndexes=bad.index.to_list()\n",
    "        #print(badIndexes)\n",
    "        data.reset_index(drop=True,inplace=True)\n",
    "        data=data.drop(badIndexes)\n",
    "        \n",
    "        return data\n",
    "#recibe el arreglo de dataframes \n",
    "##Recupera todas las materias con sus respectivos nrc\n",
    "## devuelve un dataFrame de todas la materias con su nrc\n",
    "def getCleanMaterias(data):\n",
    "    \n",
    "    materias=data[0].Materia.tolist()\n",
    "    nrcs=data[0].NRC.tolist()\n",
    "    numOfDoc=data[0].Doc.tolist()\n",
    "    \n",
    "    for x in range(len(data)-1):\n",
    "        materias.extend(data[x+1].Materia.tolist())\n",
    "        nrcs.extend(data[x+1].NRC.tolist())\n",
    "        numOfDoc.extend(data[x+1].Doc.tolist())\n",
    "        \n",
    "    antesMat=\"\"\n",
    "    antesNrc=0\n",
    "    realMaterias=[]\n",
    "    for m,n,d in zip(materias,nrcs,numOfDoc):\n",
    "        ahoraMat=m\n",
    "        ahoraNrc=n\n",
    "        if(antesNrc==-1):\n",
    "            #print('aqui----'+antesMat+' '+m,n,type(n))\n",
    "            helpDic={0:antesMat+' '+m,1:n,2:str(d)}\n",
    "            realMaterias.append(helpDic)\n",
    "        elif(ahoraNrc!=-1):\n",
    "            helpDic={0:m,1:n,2:str(d)}\n",
    "            #print(m,n,type(n))\n",
    "            realMaterias.append(helpDic)\n",
    "        antesNrc=n\n",
    "        antesMat=m\n",
    "    p=pd.DataFrame(realMaterias)\n",
    "    p=p.rename(columns={0:'Materia',1:'NRC',2:'Doc'})\n",
    "    p=p.drop_duplicates()\n",
    "    return p\n",
    "\n",
    "def get_materias(data):\n",
    "    materias=data.Materia.tolist()\n",
    "    nrcs=data.NRC.tolist()\n",
    "    \n",
    "    antesMat=\"\"\n",
    "    antesNrc=0\n",
    "    realMaterias=[]\n",
    "    for m,n in zip(materias,nrcs):\n",
    "        ahoraMat=m\n",
    "        ahoraNrc=n\n",
    "        if(antesNrc==-1):\n",
    "            #print('aqui----'+antesMat+' '+m,n,type(n))\n",
    "            helpDic={0:antesMat+' '+m,1:n}\n",
    "            realMaterias.append(helpDic)\n",
    "        elif(ahoraNrc!=-1):\n",
    "            helpDic={0:m,1:n}\n",
    "            #print(m,n,type(n))\n",
    "            realMaterias.append(helpDic)\n",
    "        antesNrc=n\n",
    "        antesMat=m\n",
    "    p=pd.DataFrame(realMaterias)\n",
    "    p=p.rename(columns={0:'Materia',1:'NRC'})\n",
    "    p=p.drop_duplicates()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "data=[]\n",
    "Materias=[]\n",
    "nombresMaterias=[]\n",
    "def init(directorio):\n",
    "    docs=getDocsinDir(directorio)\n",
    "    data=getCSVs(docs)\n",
    "    for x in range(len(data)):\n",
    "            data[x]=cleanData(data[x])\n",
    "            p=[x]*len(data[x])\n",
    "            data[x]['Doc']=p ##genera una columna que almacene a que numero de documento pertenece\n",
    "    Materias=getCleanMaterias(data)\n",
    "    nombresMaterias=Materias.Materia.unique().tolist()\n",
    "    return data\n",
    "def get_Materias_por_plan():\n",
    "    materias=[]\n",
    "    for x in data:\n",
    "        gg=get_materias(x)\n",
    "        gg.reset_index(drop=True,inplace=True)\n",
    "        none=[]\n",
    "        for x in range(len(gg.Materia.unique().tolist())):\n",
    "            none.append(None)\n",
    "        pp=dict(zip(gg.Materia.unique().tolist(),none))\n",
    "        materias.append(pp)\n",
    "        ###materias.append(gg.Materia.unique().tolist())\n",
    "        ###materias.append(gg)\n",
    "    return materias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['app/filesCSV/COM-CUATRI-PRIMAVERA2020.csv',\n",
       " 'app/filesCSV/COM-SEM-PRIMAVERA2020.csv',\n",
       " 'app/filesCSV/ICC-CUIATRI-PRIMAVERA2020.csv',\n",
       " 'app/filesCSV/ICC-SEM-PRIMAVERA2020.csv',\n",
       " 'app/filesCSV/ITI-CUATRI-PRIMAVERA2020.csv',\n",
       " 'app/filesCSV/ITI-SEM-PRIMAVERA2020.csv']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=getDocsinDir(\"app/filesCSV/\")\n",
    "data=init(\"app/filesCSV/\")\n",
    "ma=get_Materias_por_plan()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def save_materias_json(dicts:list):\n",
    "with open('ITI-6.json', 'w+') as fp:\n",
    "    json.dump(ma[5],fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_integrity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Materia</th>\n",
       "      <th>NRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lengua Extranjera III</td>\n",
       "      <td>20021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lengua Extranjera IV</td>\n",
       "      <td>20022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lengua Extranjera IV</td>\n",
       "      <td>20022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analisis y Diseno de Algoritm</td>\n",
       "      <td>29066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analisis y Diseno de Algoritm</td>\n",
       "      <td>29066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Materia    NRC\n",
       "0          Lengua Extranjera III  20021\n",
       "1           Lengua Extranjera IV  20022\n",
       "2           Lengua Extranjera IV  20022\n",
       "3  Analisis y Diseno de Algoritm  29066\n",
       "4  Analisis y Diseno de Algoritm  29066"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][[\"Materia\",\"NRC\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_integrity(cleaned,original_data):\n",
    "    conta=0\n",
    "    for x in cleaned[\"NRC\"]:\n",
    "        encontrado= False\n",
    "        for y in original_data[\"NRC\"]:\n",
    "            if x == y :\n",
    "                encontrado=True\n",
    "                pass\n",
    "\n",
    "        if not encontrado:\n",
    "            print(\"no encontrado {}\".format(x))\n",
    "        else:\n",
    "            conta+=1\n",
    "    if len(cleaned) == conta:\n",
    "        print(\"materias completas\")\n",
    "    else:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _para indexador de palabras de documento de texto:_\n",
    "        → retorna un diccionario con todas las palabras encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def simbologia\n",
    "#???? => es necesario?\n",
    "# [linea] ? [linea] => cual de las dos opciones es mejor?\n",
    "#DEFINIR CLASE Match\n",
    "#     class Match:\n",
    "#      numerodelinea=int\n",
    "#      ubicacion en la linea=[]  \n",
    "\n",
    "#para indexador de indice invertido\n",
    "    #leer todo el documento linea a linea\n",
    "    #limpiar el texto\n",
    "    #TOKENIZAR CADA PALABRA EN LA LINEA \n",
    "    #almacenar numero de palabras en la linea ????? (necesario si queremos calcular la pocision de la palabra relativa al doc)\n",
    "    #si la palabra NO esta en stop words y aun no esta en el diccionario:\n",
    "        #agregar la palabra al diccionario con su diccionario correspodiente\n",
    "        #guardar en su diccionario el numero de documento al que corresponde\n",
    "        #inicializar el primer elemnto de la lista (APARICIONES)=1 de la palabra en el diccionario del documento\n",
    "        #crear una nueva instancia match\n",
    "           #almacenar en que linea esta (NUM_LINEA)\n",
    "            #almacenar la ubicacion de la palabra  (relativo a la linea)\n",
    "                # almacenar que numero de palabra ocupa en la linea ? almacenar numero de caracter en el que inicia la palabra\n",
    "        #agregar la instancia de Match a la lista del diccionario de la palabra\n",
    "    #si la palabra ya esta en el diccionario\n",
    "        #incrementar por uno (APARICIONES)\n",
    "        #revisar el match.Linea anterior\n",
    "            #si es la misma linea en la que está\n",
    "                #actualizar la lista de apariciones\n",
    "            #sino\n",
    "                #crear una nueva instancia match\n",
    "                #agregar la instancia de Match a la lista del diccionario de la palabra\n",
    "        \n",
    "#estructura: lista de diccionarios t.q:\n",
    "\n",
    "#    {'token o palabra': {NUM_DOC:[APARICIONES,Match1,Match2],NUM_DOC:[APARICIONES,Match1,Match2]}}\n",
    "# [[,,,,][,,,,]] cada lista corrsponde a  un documento y la pocision de la lista almacena el numero de palabras en la linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Match:\n",
    "    def __init__(self,numeroLinea : int, ocurrencias: list):\n",
    "        self.linea=numeroLinea\n",
    "        self.ocurrencias=ocurrencias\n",
    "    def addOcurrencias(self, new:list):\n",
    "        for elem in new:\n",
    "            if elem not in self.ocurrencias:\n",
    "                self.ocurrencias.append(elem)\n",
    "    def getOcurrencias(self):\n",
    "        return self.ocurrencias\n",
    "    def getLinea(self):\n",
    "        return self.linea\n",
    "    def getNumOcurrencias(self):\n",
    "        return len(self.ocurrencias)\n",
    "\n",
    "#clase modificada para que retorne una lista en lugar del objeto\n",
    "class Match2():\n",
    "    def __init__(self,match:list):\n",
    "        self.linea=match[0]\n",
    "        self.ocurrencias=match[1:]\n",
    "    def getMatch(self):\n",
    "        return [self.linea,self.ocurrencias]\n",
    "    def addOcurrencias(self,new:list):\n",
    "        for elem in new:\n",
    "            if elem not in self.ocurrencias:\n",
    "                self.ocurrencias.append(elem)\n",
    "        return [self.linea,self.ocurrencias]\n",
    "    def getOcurrencias(self):\n",
    "        return self.ocurrencias[0]\n",
    "    def getLinea(self):\n",
    "        return self.linea\n",
    "    def getNumOcurrencias(self,):\n",
    "        return len(self.ocurrencias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ho4lá 5ño dfs   10921d aquíhool8    más'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "stopWords=set(stopwords.words('spanish'))\n",
    "stopWordsEngl=set(stopwords.words('english'))\n",
    "#recibe una cadena\n",
    "#retorna una cadena limpia\n",
    "def normal(string):\n",
    "    string=re.sub('_',' ',string)\n",
    "    string=re.sub(r'[^\\w\\s]+',' ',string) #elimina simbolos\n",
    "    string=re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\",' ',string) ##elimina numeros solos\n",
    "    string=re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\",' ',string) ##elimina numeros solos\n",
    "    string=string.lower()#↓↓ RETIRA DIACRITICOS MENOS ñ\n",
    "   #string = re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\",r\"\\1\",normalize( \"NFD\",string),0,re.I)\n",
    "    #string = normalize( 'NFC', string)\n",
    "    return string\n",
    "\n",
    "normal('ho4lá 5ño 1988´.dfs_?¡¿ 2 10921d aquíHOOL8 7545 :2012. 1901. más') #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rutina para indexador\n",
    "# recibe una lista de nombres de documentos \n",
    "#retorna un diccionario (la estructura del indexador)\n",
    "\n",
    "def makeIndex(docs:list):\n",
    "    headIndexer={}\n",
    "    numeroPalabrasPorDoc=[]\n",
    "    numDoc=0\n",
    "    for x in docs:\n",
    "        numeroPalabrasPorDoc.append([])\n",
    "        tokens=[]\n",
    "        doc=open(x,encoding='utf8')\n",
    "        data=doc.readlines()\n",
    "        ##para cada linea en el documento\n",
    "        for i in range(len(data)):\n",
    "            data[i]=normal(data[i])\n",
    "            lineTokenized=word_tokenize(data[i])\n",
    "            numeroPalabrasPorDoc[numDoc].append(len(lineTokenized))\n",
    "            if len(lineTokenized) > 0:\n",
    "                lineTokenized.insert(0,i) ##almacena el numero de linea al que pertenece en la cabeza de la lista\n",
    "                tokens.append(lineTokenized)\n",
    "    \n",
    "        for linea in tokens:\n",
    "            numpalabra=0\n",
    "            numlinea=linea[0]\n",
    "            for palabra in linea:\n",
    "                if type(palabra) is not int:\n",
    "                    if palabra not in stopWords and palabra not in stopWordsEngl:\n",
    "                        if palabra not in headIndexer:\n",
    "                            match=Match(numlinea,[numpalabra])\n",
    "                            headIndexer.update({palabra:{numDoc:[1,match]}})## psible bug\n",
    "                        else:\n",
    "                            #si el documento no esta en elindexador\n",
    "                            dicDocs= headIndexer.get(palabra)\n",
    "                            if numDoc not in dicDocs:\n",
    "                                match=Match(numlinea,[numpalabra])\n",
    "                                dicDocs.update({numDoc:[1,match]})\n",
    "                            #si el documento ya esta en el indexador\n",
    "                            else:\n",
    "                                headIndexer.get(palabra).get(numDoc)[0]+=1 ##actualiza a cantidad de ocurrencias\n",
    "                                matchAnterior=headIndexer.get(palabra).get(numDoc)[-1]\n",
    "                                if matchAnterior.getLinea() == numlinea:\n",
    "                                    matchAnterior.addOcurrencias([numpalabra])\n",
    "                                else:\n",
    "                                    match=Match(numlinea,[numpalabra])\n",
    "                                    headIndexer.get(palabra).get(numDoc).append(match)\n",
    "                    numpalabra+=1\n",
    "            numlinea+=1\n",
    "        numDoc+=1\n",
    "    return headIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rutina para indexador\n",
    "# recibe una lista de nombres de documentos \n",
    "#retorna un diccionario (la estructura del indexador)\n",
    "\n",
    "def makeIndex2(docs:list):\n",
    "    headIndexer2={}\n",
    "    numeroPalabrasPorDoc2=[]\n",
    "    numDoc=0\n",
    "    for x in docs:\n",
    "        numeroPalabrasPorDoc2.append([])\n",
    "        tokens=[]\n",
    "        doc=open(x,encoding='utf8')\n",
    "        data=doc.readlines()\n",
    "        ##para cada linea en el documento\n",
    "        for i in range(len(data)):\n",
    "            data[i]=normal(data[i])\n",
    "            lineTokenized=word_tokenize(data[i])\n",
    "            numeroPalabrasPorDoc2[numDoc].append(len(lineTokenized))\n",
    "            if len(lineTokenized) > 0:\n",
    "                lineTokenized.insert(0,i) ##almacena el numero de linea al que pertenece en la cabeza de la lista\n",
    "                tokens.append(lineTokenized)\n",
    "    \n",
    "        for linea in tokens:\n",
    "            numpalabra=0\n",
    "            numlinea=linea[0]\n",
    "            for palabra in linea:\n",
    "                if type(palabra) is not int:\n",
    "                    if palabra not in stopWords and palabra not in stopWordsEngl:\n",
    "                        if palabra not in headIndexer2:\n",
    "                            match=Match2([numlinea,numpalabra]).getMatch()\n",
    "                            headIndexer2.update({palabra:{numDoc:[1,match]}})## psible bug\n",
    "                        else:\n",
    "                            #si el documento no esta en elindexador\n",
    "                            dicDocs= headIndexer2.get(palabra)\n",
    "                            if numDoc not in dicDocs:\n",
    "                                match=Match2([numlinea,numpalabra]).getMatch()\n",
    "                                #print(match)\n",
    "                                dicDocs.update({numDoc:[1,match]})\n",
    "                            #si el documento ya esta en el indexador\n",
    "                            else:\n",
    "                                headIndexer2.get(palabra).get(numDoc)[0]+=1 ##actualiza la cantidad de ocurrencias\n",
    "                                matchAnterior=headIndexer2.get(palabra).get(numDoc)[-1]\n",
    "                                if matchAnterior[0] == numlinea:\n",
    "                                    matchAnterior[1].append(numpalabra)\n",
    "                                    #print(matchAnterior)\n",
    "                                else:\n",
    "                                    match=Match2([numlinea,numpalabra]).getMatch()\n",
    "                                    headIndexer2.get(palabra).get(numDoc).append(match)\n",
    "                    numpalabra+=1\n",
    "            numlinea+=1\n",
    "        numDoc+=1\n",
    "    return headIndexer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=[\"app/server.py\",\"app/indexer.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "ide=makeIndex(txt)\n",
    "isinstance(ide.get(\"main\")[0][1],Match)\n",
    "class AEncoder(json.JSONEncoder):\n",
    "    def default (self, obj):\n",
    "        if type(obj)==Match:\n",
    "            return obj.__dict__\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "with open('indexador.json', 'w+') as fp:\n",
    "    json.dump(ide,fp,cls=AEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexado=makeIndex2(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para buscar en el indexador\n",
    "    # abrir todos los documentos y guaradar todos sus apuntadores en una lista\n",
    "    # leer cada documento y en un array de lineas y alamacenarlos en una lista (hbuffer)\n",
    "    #si la palabra esta en el indexador\n",
    "        #recibir el diccionario de documentos correspondiente a la palabra\n",
    "        #recorrer cada elemento en el diccionario\n",
    "            #recorrer cada valor en la lista del elemento del diccionario\n",
    "                #si el valor en la lista es un entero(el entero es el numero de incidencias)\n",
    "                    #imprimir el numero ocurrencias y el nombre del documento\n",
    "                #sino\n",
    "                    #imprimir la lista de ocurrencias del primer match\n",
    "                    #imprimir la linea del documento original almacenada en (hbuffer)\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbuffer=[]\n",
    "for x in range(len(txt)):\n",
    "    hbuffer.append(None)\n",
    "#funcion que recibe la palabra y el indexador e imprime los rsultados de la busqueda\n",
    "def searchIndex(word,index):\n",
    "    if word in index:\n",
    "        dicDoc=index.get(word)\n",
    "        for doc in dicDoc:\n",
    "            #buffer.insert(doc,open(txt[doc],encoding='utf8'))\n",
    "            #print(buffer)\n",
    "            hbuffer[doc]=open(txt[doc],encoding='utf8').readlines()\n",
    "            for value in dicDoc[doc]:\n",
    "                if type(value) is int:\n",
    "                    print(\"{} ocurrencias en {}\".format(value,txt[doc]))\n",
    "                else:\n",
    "                    lst=list(map(lambda x: x+1,value.getOcurrencias())) #le suma uno a cada elemento de las ocurrencias\n",
    "                    print(\"\\tEn linea {} \\n\\t\\tpalabra {}\".format(value.getLinea()+1,lst))\n",
    "                    print(\"\\n\\t{}\".format(hbuffer[doc][value.getLinea()]))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbuffer2=[]\n",
    "for x in range(len(txt)):\n",
    "    hbuffer2.append(None)\n",
    "#funcion que recibe la palabra y el indexador e imprime los rsultados de la busqueda\n",
    "def searchIndex2(word,index):\n",
    "    if word in index:\n",
    "        dicDoc=index.get(word)\n",
    "        for doc in dicDoc:\n",
    "            #buffer.insert(doc,open(txt[doc],encoding='utf8'))\n",
    "            #print(buffer)\n",
    "            hbuffer2[doc]=open(txt[doc],encoding='utf8').readlines()\n",
    "            for value in dicDoc[doc]:\n",
    "                if type(value) is int:\n",
    "                    print(\"{} ocurrencias en {}\".format(value,txt[doc]))\n",
    "                else:\n",
    "                    extractor=Match2(value)\n",
    "                    #print(extractor.getOcurrencias())\n",
    "                    #print(extractor.getMatch())\n",
    "                    lst=list(map(lambda x: x+1,extractor.getOcurrencias())) #le suma uno a cada elemento de las ocurrencias\n",
    "                    print(\"\\tEn linea {} \\n\\t\\tpalabra {}\".format(extractor.getLinea()+1,lst))\n",
    "                    print(\"\\n\\t{}\".format(hbuffer2[doc][extractor.getLinea()]))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ocurrencias en app/server.py\n",
      "\tEn linea 17 \n",
      "\t\tpalabra [1]\n",
      "\n",
      "\tmain\n",
      "\n",
      "\tEn linea 29 \n",
      "\t\tpalabra [3, 4]\n",
      "\n",
      "\tif __name__=='__main__':   main\n",
      "\n",
      "1 ocurrencias en app/indexer.py\n",
      "\tEn linea 98 \n",
      "\t\tpalabra [3]\n",
      "\n",
      "\tif __name__ == '__main__':\n",
      "\n"
     ]
    }
   ],
   "source": [
    "searchIndex2(\"main\",indexado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ocurrencias en app/server.py\n",
      "\tEn linea 17 \n",
      "\t\tpalabra [1]\n",
      "\n",
      "\t\n",
      "\n",
      "\tEn linea 29 \n",
      "\t\tpalabra [3, 4]\n",
      "\n",
      "\tif __name__=='__main__':\n",
      "\n",
      "1 ocurrencias en app/indexer.py\n",
      "\tEn linea 98 \n",
      "\t\tpalabra [3]\n",
      "\n",
      "\tif __name__ == '__main__':\n",
      "\n"
     ]
    }
   ],
   "source": [
    "searchIndex(\"main\",ide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
