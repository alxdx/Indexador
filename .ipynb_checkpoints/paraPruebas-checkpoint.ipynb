{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esta notebook es creada para hacer pruebas con el procesamiento de la informacion de los documentos csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _para limpiar los documentos extraidos del pdf_\n",
    "    ->> obtener una lista de todas las materias con sus nrc y la carrera en la que se ofertan\n",
    "    --- limpiar la lista de materias (toLower)\n",
    "    --- sustituir los nombres de materias por los correctos\n",
    "    ---reemplazar los NAN donde se genera esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Materia</th>\n",
       "      <th>Clave</th>\n",
       "      <th>Secc</th>\n",
       "      <th>NRC</th>\n",
       "      <th>Dias</th>\n",
       "      <th>Hora</th>\n",
       "      <th>Salon</th>\n",
       "      <th>Profesor</th>\n",
       "      <th>Doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Inteligencia Artificial</td>\n",
       "      <td>CCOM 262</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29089</td>\n",
       "      <td>AJ</td>\n",
       "      <td>0900-1059</td>\n",
       "      <td>1EMA4/409</td>\n",
       "      <td>ZACARIAS - FLORES FERNANDO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Inteligencia Artificial</td>\n",
       "      <td>CCOM 262</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29089</td>\n",
       "      <td>L</td>\n",
       "      <td>1000-1059</td>\n",
       "      <td>1EMA4/409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Arqu. Funcional de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZACARIAS - FLORES FERNANDO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Computadora</td>\n",
       "      <td>IDCO 200</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29093</td>\n",
       "      <td>L</td>\n",
       "      <td>1300-1359</td>\n",
       "      <td>1CCO5/201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Arqu. Funcional de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SANCHEZ - GALVEZ MARIA EUGENIA N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Computadora</td>\n",
       "      <td>IDCO 200</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29093</td>\n",
       "      <td>M</td>\n",
       "      <td>1300-1459</td>\n",
       "      <td>1CCO3/309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Materia     Clave Secc    NRC Dias       Hora      Salon  \\\n",
       "16  Inteligencia Artificial  CCOM 262  OO1  29089   AJ  0900-1059  1EMA4/409   \n",
       "17  Inteligencia Artificial  CCOM 262  OO1  29089    L  1000-1059  1EMA4/409   \n",
       "18       Arqu. Funcional de       NaN  NaN     -1  NaN        NaN        NaN   \n",
       "19              Computadora  IDCO 200  OO1  29093    L  1300-1359  1CCO5/201   \n",
       "20       Arqu. Funcional de       NaN  NaN     -1  NaN        NaN        NaN   \n",
       "21              Computadora  IDCO 200  OO1  29093    M  1300-1459  1CCO3/309   \n",
       "\n",
       "                            Profesor  Doc  \n",
       "16        ZACARIAS - FLORES FERNANDO    0  \n",
       "17                               NaN    0  \n",
       "18        ZACARIAS - FLORES FERNANDO    0  \n",
       "19                               NaN    0  \n",
       "20  SANCHEZ - GALVEZ MARIA EUGENIA N    0  \n",
       "21                               NaN    0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###celda de erorres TODO\n",
    "data[0][16:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recibe un directorio\n",
    "#retorna todos los documentos de ese directorio\n",
    "def getDocsinDir(directory):\n",
    "    docs=[]\n",
    "    for root,directories,files in os.walk(directory+'/'):\n",
    "        for f in files:\n",
    "            docs.append(directory+'/'+f)\n",
    "    return docs\n",
    "#recibe una lista de nombres de documentos\n",
    "#retorna un lista de dataFrames\n",
    "def getCSVs(docs):\n",
    "    data=[]\n",
    "    for x in docs:\n",
    "        data.append(pd.read_csv(x))\n",
    "    return data\n",
    "#recibe un panda.Dataframes\n",
    "    #elimina todos los elementos nulos y ajusta los nombres de materias y profesores\n",
    "#retorna el mismo Dataframe  \n",
    "def cleanData(data):    \n",
    "        data = data[data['Materia'].notna()]\n",
    "        data['NRC']=data['NRC'].replace(to_replace=np.nan,value=-1)\n",
    "        #data[2].dropna(inplace=True)\n",
    "        data.reset_index(drop=True,inplace=True)\n",
    "        bad=data.loc[lambda y: y.Materia=='Materia']\n",
    "        #print(bad)\n",
    "        badIndexes=bad.index.to_list()\n",
    "        #print(badIndexes)\n",
    "        data.reset_index(drop=True,inplace=True)\n",
    "        data=data.drop(badIndexes)\n",
    "        \n",
    "        return data\n",
    "#recibe el arreglo de dataframes \n",
    "##Recupera todas las materias con sus respectivos nrc\n",
    "## devuelve un dataFrame de todas la materias con su nrc\n",
    "def getCleanMaterias(data):\n",
    "    \n",
    "    materias=data[0].Materia.tolist()\n",
    "    nrcs=data[0].NRC.tolist()\n",
    "    numOfDoc=data[0].Doc.tolist()\n",
    "    \n",
    "    for x in range(len(data)-1):\n",
    "        materias.extend(data[x+1].Materia.tolist())\n",
    "        nrcs.extend(data[x+1].NRC.tolist())\n",
    "        numOfDoc.extend(data[x+1].Doc.tolist())\n",
    "        \n",
    "    antesMat=\"\"\n",
    "    antesNrc=0\n",
    "    realMaterias=[]\n",
    "    for m,n,d in zip(materias,nrcs,numOfDoc):\n",
    "        ahoraMat=m\n",
    "        ahoraNrc=n\n",
    "        if(antesNrc==-1):\n",
    "            #print('aqui----'+antesMat+' '+m,n,type(n))\n",
    "            helpDic={0:antesMat+' '+m,1:n,2:str(d)}\n",
    "            realMaterias.append(helpDic)\n",
    "        elif(ahoraNrc!=-1):\n",
    "            helpDic={0:m,1:n,2:str(d)}\n",
    "            #print(m,n,type(n))\n",
    "            realMaterias.append(helpDic)\n",
    "        antesNrc=n\n",
    "        antesMat=m\n",
    "    p=pd.DataFrame(realMaterias)\n",
    "    p=p.rename(columns={0:'Materia',1:'NRC',2:'Doc'})\n",
    "    p=p.drop_duplicates()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=getDocsinDir(\"filesCSV\")\n",
    "data=getCSVs(docs)\n",
    "for x in range(len(data)):\n",
    "        data[x]=cleanData(data[x])\n",
    "        p=[x]*len(data[x])\n",
    "        data[x]['Doc']=p ##genera una columna que almacene a que numero de documento pertenece\n",
    "        \n",
    "Materias=getCleanMaterias(data)\n",
    "nombresMaterias=Materias.Materia.unique().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _para indexador de palabras de documento de texto:_\n",
    "        â†’ retorna un diccionario con todas las palabras encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def simbologia\n",
    "#???? => es necesario?\n",
    "# [linea] ? [linea] => cual de las dos opciones es mejor?\n",
    "#DEFINIR CLASE Match\n",
    "#     class Match:\n",
    "#      numerodelinea=int\n",
    "#      ubicacion en la linea=[]  \n",
    "\n",
    "#para indexador de indice invertido\n",
    "    #leer todo el documento linea a linea\n",
    "    #limpiar el texto\n",
    "    #TOKENIZAR CADA PALABRA EN LA LINEA \n",
    "    #almacenar numero de palabras en la linea ????? (necesario si queremos calcular la pocision de la palabra relativa al doc)\n",
    "    #si la palabra NO esta en stop words y aun no esta en el diccionario:\n",
    "        #agregar la palabra al diccionario con su diccionario correspodiente\n",
    "        #guardar en su diccionario el numero de documento al que corresponde\n",
    "        #inicializar el primer elemnto de la lista (APARICIONES)=1 de la palabra en el diccionario del documento\n",
    "        #crear una nueva instancia match\n",
    "           #almacenar en que linea esta (NUM_LINEA)\n",
    "            #almacenar la ubicacion de la palabra  (relativo a la linea)\n",
    "                # almacenar que numero de palabra ocupa en la linea ? almacenar numero de caracter en el que inicia la palabra\n",
    "        #agregar la instancia de Match a la lista del diccionario de la palabra\n",
    "    #si la palabra ya esta en el diccionario\n",
    "        #incrementar por uno (APARICIONES)\n",
    "        #revisar el match.Linea anterior\n",
    "            #si es la misma linea en la que estÃ¡\n",
    "                #actualizar la lista de apariciones\n",
    "            #sino\n",
    "                #crear una nueva instancia match\n",
    "                #agregar la instancia de Match a la lista del diccionario de la palabra\n",
    "        \n",
    "#estructura: lista de diccionarios t.q:\n",
    "\n",
    "#    {'token o palabra': {NUM_DOC:[APARICIONES,Match1,Match2],NUM_DOC:[APARICIONES,Match1,Match2]}}\n",
    "# [[,,,,][,,,,]] cada lista corrsponde a  un documento y la pocision de la lista almacena el numero de palabras en la linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Match:\n",
    "    def __init__(self,numeroLinea : int, ocurrencias: list):\n",
    "        self.linea=numeroLinea\n",
    "        self.ocurrencias=ocurrencias\n",
    "    def addOcurrencias(self, new:list):\n",
    "        for elem in new:\n",
    "            if elem not in self.ocurrencias:\n",
    "                self.ocurrencias.append(elem)\n",
    "    def getOcurrencias(self):\n",
    "        return self.ocurrencias\n",
    "    def getLinea(self):\n",
    "        return self.linea\n",
    "    def getNumOcurrencias(self):\n",
    "        return len(self.ocurrencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ho4lÃ¡ 5Ã±o dfs   10921d aquÃ­hool8    mÃ¡s'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "stopWords=set(stopwords.words('spanish'))\n",
    "stopWordsEngl=set(stopwords.words('english'))\n",
    "#recibe una cadena\n",
    "#retorna una cadena limpia\n",
    "def normal(string):\n",
    "    string=re.sub('_',' ',string)\n",
    "    string=re.sub(r'[^\\w\\s]+',' ',string) #elimina simbolos\n",
    "    string=re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\",' ',string) ##elimina numeros solos\n",
    "    string=re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\",' ',string) ##elimina numeros solos\n",
    "    string=string.lower()#â†“â†“ RETIRA DIACRITICOS MENOS Ã±\n",
    "   #string = re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\",r\"\\1\",normalize( \"NFD\",string),0,re.I)\n",
    "    #string = normalize( 'NFC', string)\n",
    "    return string\n",
    "\n",
    "normal('ho4lÃ¡ 5Ã±o 1988Â´.dfs_?Â¡Â¿ 2 10921d aquÃ­HOOL8 7545 :2012. 1901. mÃ¡s') #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rutina para indexador\n",
    "# recibe una lista de nombres de documentos \n",
    "#retorna un diccionario (la estructura del indexador)\n",
    "def makeIndex(docs:list):\n",
    "    headIndexer={}\n",
    "    numeroPalabrasPorDoc=[]\n",
    "    numDoc=0\n",
    "    for x in docs:\n",
    "        numeroPalabrasPorDoc.append([])\n",
    "        tokens=[]\n",
    "        doc=open(x,encoding='utf8')\n",
    "        data=doc.readlines()\n",
    "        ##para cada linea en el documento\n",
    "        for i in range(len(data)):\n",
    "            data[i]=normal(data[i])\n",
    "            lineTokenized=word_tokenize(data[i])\n",
    "            numeroPalabrasPorDoc[numDoc].append(len(lineTokenized))\n",
    "            if len(lineTokenized) > 0:\n",
    "                lineTokenized.insert(0,i) ##almacena el numero de linea al que pertenece en la cabeza de la lista\n",
    "                tokens.append(lineTokenized)\n",
    "    \n",
    "        for linea in tokens:\n",
    "            numpalabra=0\n",
    "            numlinea=linea[0]\n",
    "            for palabra in linea:\n",
    "                if type(palabra) is not int:\n",
    "                    if palabra not in stopWords and palabra not in stopWordsEngl:\n",
    "                        if palabra not in headIndexer:\n",
    "                            match=Match(numlinea,[numpalabra])\n",
    "                            headIndexer.update({palabra:{numDoc:[1,match]}})## psible bug\n",
    "                        else:\n",
    "                            #si el documento no esta en elindexador\n",
    "                            dicDocs= headIndexer.get(palabra)\n",
    "                            if numDoc not in dicDocs:\n",
    "                                match=Match(numlinea,[numpalabra])\n",
    "                                dicDocs.update({numDoc:[1,match]})\n",
    "                            #si el documento ya esta en el indexador\n",
    "                            else:\n",
    "                                headIndexer.get(palabra).get(numDoc)[0]+=1 ##actualiza a cantidad de ocurrencias\n",
    "                                matchAnterior=headIndexer.get(palabra).get(numDoc)[-1]\n",
    "                                if matchAnterior.getLinea() == numlinea:\n",
    "                                    matchAnterior.addOcurrencias([numpalabra])\n",
    "                                else:\n",
    "                                    match=Match(numlinea,[numpalabra])\n",
    "                                    headIndexer.get(palabra).get(numDoc).append(match)\n",
    "                    numpalabra+=1\n",
    "            numlinea+=1\n",
    "        numDoc+=1\n",
    "    return headIndexer,tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=['textDocs/likeAPrayer.txt','textDocs/sencillosMasVendidosDelMundo.txt','textWebPages/page0.txt',\n",
    "     \"textWebPages/page1.txt\",\"textWebPages/page2.txt\",\"textWebPages/page3.txt\",\n",
    "     \"textWebPages/page4.txt\",\"textWebPages/page5.txt\",\"textWebPages/page6.txt\",\n",
    "     \"textWebPages/page7.txt\",\"textWebPages/page8.txt\",\"textWebPages/page9.txt\"]\n",
    "indexado,tokens=makeIndex(txt)\n",
    "#indexado\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para buscar en el indexador\n",
    "    # abrir todos los documentos y guaradar todos sus apuntadores en una lista\n",
    "    # leer cada documento y en un array de lineas y alamacenarlos en una lista (hbuffer)\n",
    "    #si la palabra esta en el indexador\n",
    "        #recibir el diccionario de documentos correspondiente a la palabra\n",
    "        #recorrer cada elemento en el diccionario\n",
    "            #recorrer cada valor en la lista del elemento del diccionario\n",
    "                #si el valor en la lista es un entero(el entero es el numero de incidencias)\n",
    "                    #imprimir el numero ocurrencias y el nombre del documento\n",
    "                #sino\n",
    "                    #imprimir la lista de ocurrencias del primer match\n",
    "                    #imprimir la linea del documento original almacenada en (hbuffer)\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion que recibe la palabra y el indexador e imprime los rsultados de la busqueda\n",
    "def searchIndex(word,index):\n",
    "    buffer=[]\n",
    "    for x in txt:\n",
    "        buffer.append(open(x,encoding='utf8'))\n",
    "    hbuffer=[]\n",
    "    for x in buffer:\n",
    "        hbuffer.append(x.readlines())\n",
    "    if word in index:\n",
    "        dicDoc=index.get(word)\n",
    "        for doc in dicDoc:\n",
    "            #print(documentos[doc])\n",
    "            for value in dicDoc[doc]:\n",
    "                if type(value) is int:\n",
    "                    print(\"{} ocurrencias en {}\".format(value,txt[doc]))\n",
    "                else:\n",
    "                    lst=list(map(lambda x: x+1,value.getOcurrencias())) #le suma uno a cada elemento de las ocurrencias\n",
    "                    print(\"\\tEn linea {} \\n\\t\\tpalabra {}\".format(value.getLinea()+1,lst))\n",
    "                    print(\"\\n\\t{}\".format(hbuffer[doc][value.getLinea()]))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ocurrencias en textWebPages/page0.txt\n",
      "\tEn linea 108 \n",
      "\t\tpalabra [12]\n",
      "\n",
      "\tEl arroz o la pasta integral, la avena, el trigo, la quinoa y otros granos\n",
      "\n",
      "1 ocurrencias en textWebPages/page2.txt\n",
      "\tEn linea 101 \n",
      "\t\tpalabra [10]\n",
      "\n",
      "\tfrescos o en conserva, guisantes congelados o en conserva, quinoa fresca,\n",
      "\n",
      "3 ocurrencias en textWebPages/page6.txt\n",
      "\tEn linea 110 \n",
      "\t\tpalabra [3]\n",
      "\n",
      "\t200 gr de quinoa      100 gr de harina de garbanzos      Para rebozar      100\n",
      "\n",
      "\tEn linea 114 \n",
      "\t\tpalabra [7]\n",
      "\n",
      "\testÃ©n blandos (unos 20 minutos). Lavar la quinoa con abundante agua (hasta que\n",
      "\n",
      "\tEn linea 116 \n",
      "\t\tpalabra [5]\n",
      "\n",
      "\tpreferentemente) y agregar la quinoa lo mÃ¡s seca posible, agregar la harina de\n",
      "\n"
     ]
    }
   ],
   "source": [
    "searchIndex(\"quinoa\",indexado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexado.get(\"comida\")[5][1].getOcurrencias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexado.get(\"comida\")[5][1].getLinea()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match=Match(9,[1,2])\n",
    "listadelistas=[]\n",
    "listadelistas.append([])\n",
    "listadelistas[0].append(0)\n",
    "listadelistas.append([])\n",
    "dic={'hola':{'h':[11,12,14],'o':[45,65,900],'l':[4,9,0,88]},'pero':{'p':[1,4,6] }}\n",
    "\n",
    "#dic.get('hola').get('o')[0]+=10\n",
    "#p=nd.get('o')\n",
    "#p.extend([5,4])\n",
    "#dic.get('hola').get('o').append(12)\n",
    "#pp=dic.get('hola').get('o')[-2]\n",
    "#pp.addOcurrencias([3,4])\n",
    "#dic.get('hola').get('h')[-1].getOcurrencias()\n",
    "#r =dic.get('hola')\n",
    "#r.update({'p':[1]})\n",
    "'mÃ¡s' in stopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parte dedicada para la recuperacion de informacion de paginas web "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist=[\"[document]\",\"script\",\"header\",\"html\",\"meta\",\"head\",\"input\",\"noscript\",\"style\",\"link\"]\n",
    "infoPages=[]\n",
    "page=''\n",
    "for i in range(10):\n",
    "    fd=open(\"webPages/page\"+str(i)+\".html\",\"r\",encoding=\"utf8\")\n",
    "    html=bs4.BeautifulSoup(fd.read(),features=\"html.parser\")\n",
    "    notCleanText=html.find_all(text=True)\n",
    "    for x in notCleanText :\n",
    "        if x.parent.name not in blacklist and not isinstance(x,bs4.element.Comment):\n",
    "            page+=\"{}\".format(x)\n",
    "    infoPages.append(page)\n",
    "    page=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tt=infoPages[1].splitlines()\n",
    "new=[]\n",
    "for i in tt:\n",
    "    if(len(i)>0):\n",
    "        if(len(i)>100):\n",
    "            new.extend(textwrap.wrap(i,width=80,replace_whitespace=False))\n",
    "        else:\n",
    "            new.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    fd=open(\"textwebPages/page\"+str(i)+\".txt\",\"w+\",encoding=\"utf8\")\n",
    "    lines=infoPages[i].splitlines()\n",
    "    new=[]\n",
    "    for line in  lines:\n",
    "        if(len(line)>0):\n",
    "            if(len(line)>100):\n",
    "                new.extend(textwrap.wrap(line,width=80,replace_whitespace=False))\n",
    "            else:\n",
    "                new.append(line)\n",
    "    for x in new:\n",
    "        fd.write(x+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
