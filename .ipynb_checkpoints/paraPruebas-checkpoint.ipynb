{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esta notebook es creada para hacer pruebas con el procesamiento de la informacion de los documentos csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _para limpiar los documentos extraidos del pdf_\n",
    "    ->> obtener una lista de todas las materias con sus nrc y la carrera en la que se ofertan\n",
    "    --- limpiar la lista de materias (toLower)\n",
    "    --- sustituir los nombres de materias por los correctos\n",
    "    ---reemplazar los NAN donde se genera esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Materia</th>\n",
       "      <th>Clave</th>\n",
       "      <th>Secc</th>\n",
       "      <th>NRC</th>\n",
       "      <th>Dias</th>\n",
       "      <th>Hora</th>\n",
       "      <th>Salon</th>\n",
       "      <th>Profesor</th>\n",
       "      <th>Doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Inteligencia Artificial</td>\n",
       "      <td>CCOM 262</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29089</td>\n",
       "      <td>AJ</td>\n",
       "      <td>0900-1059</td>\n",
       "      <td>1EMA4/409</td>\n",
       "      <td>ZACARIAS - FLORES FERNANDO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Inteligencia Artificial</td>\n",
       "      <td>CCOM 262</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29089</td>\n",
       "      <td>L</td>\n",
       "      <td>1000-1059</td>\n",
       "      <td>1EMA4/409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Arqu. Funcional de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZACARIAS - FLORES FERNANDO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Computadora</td>\n",
       "      <td>IDCO 200</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29093</td>\n",
       "      <td>L</td>\n",
       "      <td>1300-1359</td>\n",
       "      <td>1CCO5/201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Arqu. Funcional de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SANCHEZ - GALVEZ MARIA EUGENIA N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Computadora</td>\n",
       "      <td>IDCO 200</td>\n",
       "      <td>OO1</td>\n",
       "      <td>29093</td>\n",
       "      <td>M</td>\n",
       "      <td>1300-1459</td>\n",
       "      <td>1CCO3/309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Materia     Clave Secc    NRC Dias       Hora      Salon  \\\n",
       "16  Inteligencia Artificial  CCOM 262  OO1  29089   AJ  0900-1059  1EMA4/409   \n",
       "17  Inteligencia Artificial  CCOM 262  OO1  29089    L  1000-1059  1EMA4/409   \n",
       "18       Arqu. Funcional de       NaN  NaN     -1  NaN        NaN        NaN   \n",
       "19              Computadora  IDCO 200  OO1  29093    L  1300-1359  1CCO5/201   \n",
       "20       Arqu. Funcional de       NaN  NaN     -1  NaN        NaN        NaN   \n",
       "21              Computadora  IDCO 200  OO1  29093    M  1300-1459  1CCO3/309   \n",
       "\n",
       "                            Profesor  Doc  \n",
       "16        ZACARIAS - FLORES FERNANDO    0  \n",
       "17                               NaN    0  \n",
       "18        ZACARIAS - FLORES FERNANDO    0  \n",
       "19                               NaN    0  \n",
       "20  SANCHEZ - GALVEZ MARIA EUGENIA N    0  \n",
       "21                               NaN    0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###celda de erorres TODO\n",
    "data[0][16:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recibe un directorio\n",
    "#retorna todos los documentos de ese directorio\n",
    "def getDocsinDir(directory):\n",
    "    docs=[]\n",
    "    for root,directories,files in os.walk(directory+'/'):\n",
    "        for f in files:\n",
    "            docs.append(directory+'/'+f)\n",
    "    return docs\n",
    "#recibe una lista de nombres de documentos\n",
    "#retorna un lista de dataFrames\n",
    "def getCSVs(docs):\n",
    "    data=[]\n",
    "    for x in docs:\n",
    "        data.append(pd.read_csv(x))\n",
    "    return data\n",
    "#recibe un panda.Dataframes\n",
    "    #elimina todos los elementos nulos y ajusta los nombres de materias y profesores\n",
    "#retorna el mismo Dataframe  \n",
    "def cleanData(data):    \n",
    "        data = data[data['Materia'].notna()]\n",
    "        data['NRC']=data['NRC'].replace(to_replace=np.nan,value=-1)\n",
    "        #data[2].dropna(inplace=True)\n",
    "        data.reset_index(drop=True,inplace=True)\n",
    "        bad=data.loc[lambda y: y.Materia=='Materia']\n",
    "        #print(bad)\n",
    "        badIndexes=bad.index.to_list()\n",
    "        #print(badIndexes)\n",
    "        data.reset_index(drop=True,inplace=True)\n",
    "        data=data.drop(badIndexes)\n",
    "        \n",
    "        return data\n",
    "#recibe el arreglo de dataframes \n",
    "##Recupera todas las materias con sus respectivos nrc\n",
    "## devuelve un dataFrame de todas la materias con su nrc\n",
    "def getCleanMaterias(data):\n",
    "    \n",
    "    materias=data[0].Materia.tolist()\n",
    "    nrcs=data[0].NRC.tolist()\n",
    "    numOfDoc=data[0].Doc.tolist()\n",
    "    \n",
    "    for x in range(len(data)-1):\n",
    "        materias.extend(data[x+1].Materia.tolist())\n",
    "        nrcs.extend(data[x+1].NRC.tolist())\n",
    "        numOfDoc.extend(data[x+1].Doc.tolist())\n",
    "        \n",
    "    antesMat=\"\"\n",
    "    antesNrc=0\n",
    "    realMaterias=[]\n",
    "    for m,n,d in zip(materias,nrcs,numOfDoc):\n",
    "        ahoraMat=m\n",
    "        ahoraNrc=n\n",
    "        if(antesNrc==-1):\n",
    "            #print('aqui----'+antesMat+' '+m,n,type(n))\n",
    "            helpDic={0:antesMat+' '+m,1:n,2:str(d)}\n",
    "            realMaterias.append(helpDic)\n",
    "        elif(ahoraNrc!=-1):\n",
    "            helpDic={0:m,1:n,2:str(d)}\n",
    "            #print(m,n,type(n))\n",
    "            realMaterias.append(helpDic)\n",
    "        antesNrc=n\n",
    "        antesMat=m\n",
    "    p=pd.DataFrame(realMaterias)\n",
    "    p=p.rename(columns={0:'Materia',1:'NRC',2:'Doc'})\n",
    "    p=p.drop_duplicates()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=getDocsinDir(\"filesCSV\")\n",
    "data=getCSVs(docs)\n",
    "for x in range(len(data)):\n",
    "        data[x]=cleanData(data[x])\n",
    "        p=[x]*len(data[x])\n",
    "        data[x]['Doc']=p ##genera una columna que almacene a que numero de documento pertenece\n",
    "        \n",
    "Materias=getCleanMaterias(data)\n",
    "nombresMaterias=Materias.Materia.unique().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _para indexador de palabras de documento de texto:_\n",
    "        → retorna un diccionario con todas las palabras encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def simbologia\n",
    "#???? => es necesario?\n",
    "# [linea] ? [linea] => cual de las dos opciones es mejor?\n",
    "#DEFINIR CLASE Match\n",
    "#     class Match:\n",
    "#      numerodelinea=int\n",
    "#      ubicacion en la linea=[]  \n",
    "\n",
    "#para indexador de indice invertido\n",
    "    #leer todo el documento linea a linea\n",
    "    #limpiar el texto\n",
    "    #TOKENIZAR CADA PALABRA EN LA LINEA \n",
    "    #almacenar numero de palabras en la linea ????? (necesario si queremos calcular la pocision de la palabra relativa al doc)\n",
    "    #si la palabra NO esta en stop words y aun no esta en el diccionario:\n",
    "        #agregar la palabra al diccionario con su diccionario correspodiente\n",
    "        #guardar en su diccionario el numero de documento al que corresponde\n",
    "        #inicializar el primer elemnto de la lista (APARICIONES)=1 de la palabra en el diccionario del documento\n",
    "        #crear una nueva instancia match\n",
    "           #almacenar en que linea esta (NUM_LINEA)\n",
    "            #almacenar la ubicacion de la palabra  (relativo a la linea)\n",
    "                # almacenar que numero de palabra ocupa en la linea ? almacenar numero de caracter en el que inicia la palabra\n",
    "        #agregar la instancia de Match a la lista del diccionario de la palabra\n",
    "    #si la palabra ya esta en el diccionario\n",
    "        #incrementar por uno (APARICIONES)\n",
    "        #revisar el match.Linea anterior\n",
    "            #si es la misma linea en la que está\n",
    "                #actualizar la lista de apariciones\n",
    "            #sino\n",
    "                #crear una nueva instancia match\n",
    "                #agregar la instancia de Match a la lista del diccionario de la palabra\n",
    "        \n",
    "#estructura: lista de diccionarios t.q:\n",
    "\n",
    "#    {'token o palabra': {NUM_DOC:[APARICIONES,Match1,Match2],NUM_DOC:[APARICIONES,Match1,Match2]}}\n",
    "# [[,,,,][,,,,]] cada lista corrsponde a  un documento y la pocision de la lista almacena el numero de palabras en la linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Match:\n",
    "    def __init__(self,numeroLinea : int, ocurrencias: list):\n",
    "        self.linea=numeroLinea\n",
    "        self.ocurrencias=ocurrencias\n",
    "    def addOcurrencias(self, new:list):\n",
    "        for elem in new:\n",
    "            if elem not in self.ocurrencias:\n",
    "                self.ocurrencias.append(elem)\n",
    "    def getOcurrencias(self):\n",
    "        return self.ocurrencias\n",
    "    def getLinea(self):\n",
    "        return self.linea\n",
    "    def getNumOcurrencias(self):\n",
    "        return len(self.ocurrencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ho4lá 5ño dfs   10921d aquíhool8    más'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "stopWords=set(stopwords.words('spanish'))\n",
    "stopWordsEngl=set(stopwords.words('english'))\n",
    "#recibe una cadena\n",
    "#retorna una cadena limpia\n",
    "def normal(string):\n",
    "    string=re.sub('_',' ',string)\n",
    "    string=re.sub(r'[^\\w\\s]+',' ',string) #elimina simbolos\n",
    "    string=re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\",' ',string) ##elimina numeros solos\n",
    "    string=re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\",' ',string) ##elimina numeros solos\n",
    "    string=string.lower()#↓↓ RETIRA DIACRITICOS MENOS ñ\n",
    "   #string = re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\",r\"\\1\",normalize( \"NFD\",string),0,re.I)\n",
    "    #string = normalize( 'NFC', string)\n",
    "    return string\n",
    "\n",
    "normal('ho4lá 5ño 1988´.dfs_?¡¿ 2 10921d aquíHOOL8 7545 :2012. 1901. más') #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rutina para indexador\n",
    "# recibe una lista de nombres de documentos \n",
    "#retorna un diccionario (la estructura del indexador)\n",
    "def makeIndex(docs:list):\n",
    "    headIndexer={}\n",
    "    numeroPalabrasPorDoc=[]\n",
    "    numDoc=0\n",
    "    for x in docs:\n",
    "        numeroPalabrasPorDoc.append([])\n",
    "        tokens=[]\n",
    "        doc=open(x,encoding='utf8')\n",
    "        data=doc.readlines()\n",
    "        ##para cada linea en el documento\n",
    "        for i in range(len(data)):\n",
    "            data[i]=normal(data[i])\n",
    "            lineTokenized=word_tokenize(data[i])\n",
    "            numeroPalabrasPorDoc[numDoc].append(len(lineTokenized))\n",
    "            if len(lineTokenized) > 0:\n",
    "                lineTokenized.insert(0,i) ##almacena el numero de linea al que pertenece\n",
    "                tokens.append(lineTokenized)\n",
    "    \n",
    "        for linea in tokens:\n",
    "            numpalabra=0\n",
    "            numlinea=linea[0]\n",
    "            for palabra in linea:\n",
    "                if type(palabra) is not int:\n",
    "                    if palabra not in stopWords and palabra not in stopWordsEngl:\n",
    "                        if palabra not in headIndexer:\n",
    "                            match=Match(numlinea,[numpalabra])\n",
    "                            headIndexer.update({palabra:{numDoc:[1,match]}})## psible bug\n",
    "                        else:\n",
    "                            #si el documento no esta en elindexador\n",
    "                            dicDocs= headIndexer.get(palabra)\n",
    "                            if numDoc not in dicDocs:\n",
    "                                dicDocs.update({numDoc:[1,match]})\n",
    "                            #si el documento ya esta en el indexador\n",
    "                            else:\n",
    "                                headIndexer.get(palabra).get(numDoc)[0]+=1\n",
    "                                matchAnterior=headIndexer.get(palabra).get(numDoc)[-1]\n",
    "                                if matchAnterior.getLinea() == numlinea:\n",
    "                                    matchAnterior.addOcurrencias([numpalabra])\n",
    "                                else:\n",
    "                                    match=Match(numlinea,[numpalabra])\n",
    "                                    headIndexer.get(palabra).get(numDoc).append(match)\n",
    "                    numpalabra+=1\n",
    "            numlinea+=1\n",
    "        numDoc+=1\n",
    "    return headIndexer,tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=['textDocs/likeAPrayer.txt','textDocs/sencillosMasVendidosDelMundo.txt','textWebPages/page0.txt',\n",
    "     \"textWebPages/page1.txt\",\"textWebPages/page2.txt\",\"textWebPages/page3.txt\",\n",
    "     \"textWebPages/page4.txt\",\"textWebPages/page5.txt\",\"textWebPages/page6.txt\",\n",
    "     \"textWebPages/page7.txt\",\"textWebPages/page8.txt\",\"textWebPages/page9.txt\"]\n",
    "indexado,tokens=makeIndex(txt)\n",
    "#indexado\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchIndex(word,index):\n",
    "    buffer=[]\n",
    "    for x in txt:\n",
    "        buffer.append(open(x,encoding='utf8'))\n",
    "    hbuffer=[]\n",
    "    for x in buffer:\n",
    "        hbuffer.append(x.readlines())\n",
    "        \n",
    "    if word in index:\n",
    "        dicDoc=index.get(word)\n",
    "        for doc in dicDoc:\n",
    "            #print(documentos[doc])\n",
    "            for value in dicDoc[doc]:\n",
    "                if type(value) is int:\n",
    "                    print(\"{} ocurrencias en {}\".format(value,txt[doc]))\n",
    "                else:\n",
    "                    lst=list(map(lambda x: x+1,value.getOcurrencias()))\n",
    "                    print(\"\\tEn linea {} \\n\\t\\tpalabra {}\".format(value.getLinea()+1,lst))\n",
    "                    print(\"\\n\\t{}\".format(hbruffer[doc][value.getLinea()]))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ocurrencias en textWebPages/page1.txt\n",
      "\tEn linea 616 \n",
      "\t\tpalabra [9]\n",
      "\n",
      "\tLos ingredientes que no pueden faltar en una cocina saludable\n",
      "\n",
      "1 ocurrencias en textWebPages/page2.txt\n",
      "\tEn linea 370 \n",
      "\t\tpalabra [1]\n",
      "\n",
      "\tAlimentos\n",
      "\n",
      "4 ocurrencias en textWebPages/page3.txt\n",
      "\tEn linea 108 \n",
      "\t\tpalabra [4]\n",
      "\n",
      "\tLas Recetas de Comida y Cocina Fáciles, de toda la vida.\n",
      "\n",
      "\tEn linea 126 \n",
      "\t\tpalabra [3]\n",
      "\n",
      "\tRecetas de Cocina 7 recetas de comida saludables para esta cuarentena\n",
      "\n",
      "\tEn linea 130 \n",
      "\t\tpalabra [3]\n",
      "\n",
      "\tRecetas de Cocina\n",
      "\n",
      "\tEn linea 329 \n",
      "\t\tpalabra [3]\n",
      "\n",
      "\t15 recetas de cocina para Cuaresma que no...\n",
      "\n",
      "5 ocurrencias en textWebPages/page4.txt\n",
      "\tEn linea 192 \n",
      "\t\tpalabra [44]\n",
      "\n",
      "\t8. Apoya a productores locales para hacer tu súper, encontrarás muchas opciones de canastas de alimentos básicos para tu semana. Y los fines, descansa un poco y pide comida de restaurantes pequeños cercanos a ti. Activa la economía local si está dentro de tu alcance.\n",
      "\n",
      "\tEn linea 193 \n",
      "\t\tpalabra [17, 23]\n",
      "\n",
      "\t9. Cocina una vez a la semana o dos para que no te sientas atado a la cocina. Así también solo limpias la cocina pocas veces y te da tiempo de otras cosas.\n",
      "\n",
      "\tEn linea 205 \n",
      "\t\tpalabra [1]\n",
      "\n",
      "\tcocina consejos cuarentena dieta rutina verdura \n",
      "\n",
      "\tEn linea 346 \n",
      "\t\tpalabra [7]\n",
      "\n",
      "\t7 datos que debes conocer sobre la cocina tradicional mexicana\n",
      "\n",
      "2 ocurrencias en textWebPages/page5.txt\n",
      "\tEn linea 577 \n",
      "\t\tpalabra [18]\n",
      "\n",
      "\tEn cuarentena obligatoria por coronavirus el tiempo sobra y se vuelve el aliado ideal para saldar deudas pendientes con la cocina. Si bien hacer pan casero no es una tarea compleja , el factor tiempo muchas veces nos saca las ganas.\n",
      "\n",
      "\tEn linea 634 \n",
      "\t\tpalabra [1]\n",
      "\n",
      "\t cocina\n",
      "\n",
      "4 ocurrencias en textWebPages/page6.txt\n",
      "\tEn linea 1025 \n",
      "\t\tpalabra [10]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-58f2da1058a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msearchIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cocina'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindexado\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-c05b11884b8b>\u001b[0m in \u001b[0;36msearchIndex\u001b[1;34m(word, index)\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mlst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOcurrencias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\tEn linea {} \\n\\t\\tpalabra {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLinea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\t{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLinea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "searchIndex('cocina',indexado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match=Match(9,[1,2])\n",
    "listadelistas=[]\n",
    "listadelistas.append([])\n",
    "listadelistas[0].append(0)\n",
    "listadelistas.append([])\n",
    "dic={'hola':{'h':[11,12,14],'o':[45,65,900],'l':[4,9,0,88]},'pero':{'p':[1,4,6] }}\n",
    "\n",
    "#dic.get('hola').get('o')[0]+=10\n",
    "#p=nd.get('o')\n",
    "#p.extend([5,4])\n",
    "#dic.get('hola').get('o').append(12)\n",
    "#pp=dic.get('hola').get('o')[-2]\n",
    "#pp.addOcurrencias([3,4])\n",
    "#dic.get('hola').get('h')[-1].getOcurrencias()\n",
    "#r =dic.get('hola')\n",
    "#r.update({'p':[1]})\n",
    "'más' in stopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parte dedicada para la recuperacion de informacion de paginas web "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist=[\"[document]\",\"script\",\"header\",\"html\",\"meta\",\"head\",\"input\",\"noscript\",\"style\",\"link\"]\n",
    "infoPages=[]\n",
    "page=''\n",
    "for i in range(10):\n",
    "    fd=open(\"webPages/page\"+str(i)+\".html\",\"r\",encoding=\"utf8\")\n",
    "    html=bs4.BeautifulSoup(fd.read(),features=\"html.parser\")\n",
    "    notCleanText=html.find_all(text=True)\n",
    "    for x in notCleanText :\n",
    "        if x.parent.name not in blacklist and not isinstance(x,bs4.element.Comment):\n",
    "            page+=\"{}\".format(x)\n",
    "    infoPages.append(page)\n",
    "    page=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    fd=open(\"textwebPages/page\"+str(i)+\".txt\",\"w+\",encoding=\"utf8\")\n",
    "    fd.write(infoPages[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
